0126_01 : learning_late=0.0001 / conv4 + pool4 + dense4 / 정확도 0.2828

0127_01 : learning_late=0.0002 / 정확도 0.2631
0127_02 : learning_late=0.00015 / 정확도 0.2644
0127_03 : learning_late=0.00005 / 정확도 0.2937
▲0127_04 : learning_late=0.00005 / dense층 1개 더 추가 / 정확도 0.2966

0128_01 : learning_late=0.0001 / 정확도 0.2191
0128_02 : learning_late=0.00003 / 정확도 0.2075 / 너무 확 내려가서 한번만 더 해봤지만 결과는 비슷했다.
0128_03 : learning_late=0.00004 / 정확도 0.2200
0128_04 : learning_late=0.00006 / 정확도 0.2875

0201_01: learning_late=0.0001 / conv층을 1개 제거하고 dense층을 1개 더 추가해...? / 정확도 0.2356 / 왜 떨어지는데
0203_01: learning_late=0.00005 / 정확도 0.1903 / 어째서
0203_02: learning_late=0.00015 / 정확도 0.2291
0203_03: learning_late=0.0002 / 정확도 0.2316
0203_04: learning_late=0.0003 / 정확도 0.2062
0203_05: learning_late=0.00025 / 정확도 0.2134
0203_06: learning_late=0.00028 / 정확도 0.2206

0206_01: learning_late=0.00029 / 정확도 0.2212
0206_02: learning_late=0.0003으로 다시 / 학습하다가 연결이 끊겼다...

다음 learning_late=0.00029


===========무신사 데이터셋=============
▲0602_01 : 기존 최고모델인 0127_04와 마찬가지로 conv4 + pool4 + dense5 / learning_late=0.00005 / 정확도 0.3051 / 그렇게 드라마틱하진 않네.....

0605_01 : 대망의 흑백이다. 이거 정확도 안오르면 어떡하지.... / 층 개수랑 하이퍼 파라미터는 똑같이 / 정확도 0.2916 / 어째서
0605_02 : learning_late-0.0005 / 정확도 0.2875

0606_01_summer : 여름의상만 따로 빼서 학습해보자. learning_late=0.00005 / 정확도 0.4233 / 계절분리가 답이었어!!!!!
0606_02_winter : 겨울의상만 / learning_late=0.00005 동일하게 / 정확도 0.3027 / 비슷하구만
0606_03_spring_autumn : 봄가을 합쳐서 / learning_late=0.00005 동일하게 / 정확도 0.2723 / 왜 가장 낮은거야
0606_04_season : 계절 판단하는 모델 / learning_late=0.00005 동일하게~~ / 정확도 0.4679 / 좀 낮군
0606_05_season : 봄/가을을 통합해보자 / 정확도 0.6116 / 애매하네....

내일은 모델 학습보다는 모델 평가만 해보자
▲계절분리 통합 전체 정확도 대충 0.3029 / 애매하다.....ㅠ

0609_01_season : 이름 수정하는거 까먹었다. 배경제거 안하고 해보자. 모델 구조는 똑같이 conv4+pool4+dense5 / learning_late=0.00005 / 정확도 0.4441 / 진짜 늘었네
▲0609_02 : 마지막 conv+pool층에 conv 하나 더 추가 / learning_late=0.00002 / 정확도 0.4744 / 뭔가 허무하지만 정확도 개선방법을 찾았으니까 뭐...

0614_01 : 구글넷 방식 / learning_late=0.00002 / 정확도 0.4682 / 흐으음
0614_02 : 구글넷인데 각 층을 더 깊게 / learning_late=0.00002 / 정확도 0.1693 / 깊다고 좋은게 아닌가
0614_03 : 원래 구글넷 / learining_late=0.00001 / 정확도 0.4482 / 구글넷이 정확도가 더 낮네......

0628_01 : 배경 정보 렉트좌표로 크롭하고, 회전도 4번 해서 어그멘테이션도 했다. 모델 형식은 시간 빠른 구글넷. learning_late=0.00002 / 정확도 0.3782 / 왜 줄은거야 역시 배경으로 정확도 좋아진거였나
0628_02 : learning_late=0.00001 / 정확도 0.3883 / 어째서

★0723_01 : 구글넷인데 마지막에 dense 레이어 추가 / learning_late=0.00002 / 정확도 0.3907 / 약간 늘긴했는데 0.4의 벽이 높다...
0723_02 : 처음에 conv층 구조를 없애고 대신 인셉션 블록으로 대체 / learining_late=0.00002 / 정확도 0.3766 / 시간만 3배정도 오래 걸리고 쓸데없어
0723_03 : 초반에 pool 레이어 다시 살려봄 / learning_late=0.00002 / 정확도 0.3669 / 모델 구조를 아예 싹 갈아엎어야하나
0723_04 : 엑스트라 네트워크도 살려봐? / learning_late=0.00002 / 결과가 여러개 나오는데 이거 어떻게 보는거냐
0723_05 : 드롭아웃층 조금만 추가 / learning_late=0.00002 / 정확도 0.3628
0723_06 : 마지막 레이어에 dense층 조금만 더 추가하자. 이것도 안되면 그냥 계절분리하는걸로 / learning_late=0.00002 / 정확도 0.3531 / 으아아악

0724_01 : pool층 다시 없애보자. dense층은 0723_01처럼 2개만. 하지만 드롭아웃 층 추가한건 유지 / learning_late=0.00002 / 정확도 0.3022 / 그냥 계절분리하자
0724_02 : 계절분리 하기전에 vgg 모델 테스트 / 형식은 con4v+pool4+ dense5 / learning_late=0.00002 / 정확도 0.2957 / 왜 더 낮아졌어 시간도 오래 걸리면서

0801_01 : 기준점 잡는 용도로 진짜진짜 기본 구글넷 / learning_late=0.0002 / 정확도 0.2728
0801_02_season : 계절분리, 기본 구글넷 / learning_late=0.00002 / 정확도 0.6317 / 더 올려야한다
0801_03_season : 처음 pool 레이어 비활성화 / learning_late=0.00002 / 정확도 0.6366 / 좀 더 올려야
0801_04_season : dense 레이어까지 2개 추가 / learning_late=0.00002 / 정확도 0.6549 / 더 오르긴했는데
0801_05_season : dense 레이어 5개까지 추가해볼까. 드롭아웃은 마지막에만 넣고 / learning_late=0.00002 / 정확도 0.6503 / 내려갔네
0801_06_season : 정확도 최고인 dense 레이어 2개 / learning_late=0.00005 / 정확도 0.6556 / 올라갔다
0801_07_season : learning_late=0.0001 / 정확도 0.6490 / 역시 드라마틱하게 올라가는 일은 없구만
0803_01_spring_autumn : 봄가을 합쳐서 / 기본 구글넷 / learning_late=0.00002 / 정확도 0.2975 / 왜 이렇게 낮아졌어
0803_02_spring_autumn : 초반 pool 없애기 / learning_late=0.00002 / 정확도 0.2518 / 더 낮아졌는데요
0803_03_spring_autumn : 초반 pool 없애고 마지막 dense층 2개 / learning_late=0.00002 / 정확도 0.2931 / 안되겠다
0803_04_spring_autumn : 마지막으로 vgg형식 써보자 / learning_late=0.00002 / 정확도 0.1124 / 에휴 계절분리는 그만두자

0804_01 : 구글넷은 변형방식에 한계가 있는 것 같다. 계절통합하고 vgg 재도전 / 기존 conv4+pool4+dense5에서 마지막 conv층 1개 더 추가 / learning_late=0.00002 / 정확도 0.1634 / 낮잖아
0804_02 : vgg, 마지막 conv층 다시 이전층이랑 똑같이 복구 / learning_late=0.00002 / 정확도 0.2153 / 조금 늘긴했는데
0804_03 : 층이 많다고 좋은게 아닌가. 마지막 dense층 4개에서 3개로 변경 / learning_late=0.00002 / 정확도 0.2654 / 오 좀 늘었다?
0804_04 : 모든 드롭아웃 비율을 0.2로 변경. 층 구조는 동일 / learning_late=0.00002 / 정확도 0.3011 / 늘었다
0804_05 : conv+pool 층 1개 줄여서 3개씩으로 만들어보자. flatten 직전의 특성맵이 25x25가 되도록.. 자꾸 오류가 나니 어그멘테이션을 2배로만 해봐야지 / learning_late=0.00002 / 정확도 0.2781 / 좀 낮아졌다
0804_06 : 반대로 conv+pool 층 1개 늘려서 flatten 직전에 6x6으로 만들보자 / learning_late=0.00002 / 정확도 0.3459 / 왜 늘었냐

0805_01 : 구글넷에서 dense층 2개도 그대로 둔 채 중간중간 드롭아웃만 조정해보자. 드롭아웃 비율은 0.2로 통일 / learning_late=0.00002 / 정확도 0.3095 / 왜 낮아
0805_02 : 구글넷인데 flatten 직전에 pooling이랑 드롭아웃 넣어서 특성맵 크기를 줄여보자. 드롭아웃은 다시 없애고 / 정확도 0.3399 / 오히려 기존보다 떨어졌네..

0811_01 : 어그멘테이션 2배 적용, 기본 ResNet50 / learning_late=0.000002 / 정확도 0.3452 / 기본인데 이정도??? 전망이 나름 좋다
0811_02 : 어그멘테이션 2배 적용, 기본 모바일넷 . learning_late=0.00002 / 정확도 0.2900 / 구글넷보다 3배나 빠르긴한데 음...
0811_03 : 어그멘테이션 2배 적용, 기본 DenseNet121 / learning_late=0.00002 / 정확도 0.3466 / 기본치고는 정확도가 높긴한데 그렇게 차이가 안나서...
0811_04 : 어그멘테이션 2배 적용, 기본 NASNetLarge / learning_late=0.00002 / 정확도 0.1445 / 시간은 5배정도로 엄청 오래 걸리는데 정확도는 영.....

0814_01 : 어그멘테이션 2배, 기본 인셉션V3 / learning_late=0.00002 / 정확도 0.3433 / 속도 비슷하네
0814_02 : 어그멘테이션 2배, 기본 인셉션resnetV2 / learning_late=0.00002 / 정확도 0.3447 / 얘도 비슷하네
0814_03 : 어그멘테이션 4배, 기본 VGG16 / learning_late=0.00002 / 정확도 0.3853 / 높긴한데 어그멘테이션 4배 감안해야돼
0814_04 : 어그멘테이션 4배, 기본 구글넷 / learning_late=0.00002 / 정확도 0.3747 / 처음에 pool층 안빼면 역시 낮다
0814_05 : 어그멘테이션 4배, 기본 ResNet50 / learning_late=0.00002 / 정확도 0.3201 / 왜 어그멘테이션 2배보다 정확도가 낮냐
0814_06 : 어그멘테이션 4배, 기본 모바일넷 / learning_late=0.00002 / 정확도 0.2941 / 낮다.. 속도는 다른 모델들이랑 비슷하게 3분정도 걸리더라
0814_07 : 어그멘테이션 4배, 기본 DenseNet121 / learning_late=0.00002 / 정확도 0.3685 / 흠...

NASNetLarge는 어그멘테이션 4배로 하니까 메모리를 너무 많이 잡아먹어서 오류가 난다...

0815_01 : 어그멘테이션 4배, 기본 인셉션V3 / learning_late=0.00002 / 정확도 0.3336
0815_02 : 어그멘테이션 4배, 기본 인셉션ResNet / learning_late=0.00002 / 정확도 0.3217 / 낮다
0815_03 : NAN가 어그멘테이션 4배로는 학습이 안돼서... 어그멘테이션 2배로 기본 VGG / learning_late=0.00002 / 정확도 0.3866
0815_04 : 어그멘테이션 2배, 기본 구글넷 / learning_late=0.00002 / 정확도 0.3842 / 꽤 높긴하네

0816_01 : 어그멘테이션 2배, 기본 VGG19 / learning_late=0.00002 / 정확도 0.3912
0816_02 : 어그멘테이션 2배, 기본 ResNet101 / learning_late=0.00002 / 정확도 0.3396
0816_03 : 어그멘테이션 2배, 기본 ResNet152 / learning_late=0.00002 / 정확도 0.3409

0817_01 : 어그멘테이션 2배, 기본 ResNet50V2 / learning_late=0.00002 / 정확도 0.3155
0817_02 : 어그멘테이션 2배, 기본 ResNet101V2 / learning_late=0.00002 / 정확도 0.3228
0817_03 : 어그멘테이션 2배, 기본 ResNet152V2 / learning_late=0.00002 / 정확도 0.3225
0817_04 : 어그멘테이션 2배, 기본 모바일넷V2 / learning_late=0.00002 / 정확도 0.2835
0817_05 : 어그멘테이션 2배, 기본 모바일넷V3Small / learning_late=0.00002 / 정확도 0.2603
0817_06 : 어그멘테이션 2배, 기본 모바일넷V3Large / learning_late=0.00002 / 정확도 0.2803
0817_07 : 어그멘테이션 2배, 기본 DenseNet169 / learning_late=0.00002 / 정확도 0.3617
0817_08 : 어그멘테이션 2배, 기본 DenseNet201 / learning_late=0.00002 / 정확도 0.3677
0817_09 : 어그멘테이션 2배, 기본 NasNetMobile / learning_late=0.00002 / 정확도 0.1518


분류
VGG19 / 0.3912
GoogLeNet / 0.3842
DenseNet / 0.3677
ResNet / 0.3452
InceptionResNetV2 / 0.3447
InceptionV3 / 0.3433
MobileNet / 0.2900
NasNetMobile / 0.1518