0122_01 : 정확도 0.2465 / learning_late를 조정해볼까

0122_02 : learning_late=0.0001 / 정확도 0.3005


0123_01 : learning_late=0.0005 / 정확도 0.2870
0123_02 : learning_late=0.0001 / conv층 추가, dense 필터수 변경 / 정확도 0.3600 / 층을 더 추가해야하나..?
▲0123_03 : leraning_late=0.0001 / conv층 필터수 2배 / 정확도 0.3745

0124_01 : learning_late=0.0001 / conv층 5개 + dense층 5개로 변경 / 정확도 0.0985 / 과대적합인가...
0124_02 : learning_late=0.0002 / conv층 4개 + dense층 4개로 변경. 그리고 loss가 조금씩 줄어서 원래 시간이 오래 걸리는 거였다. 느긋하게 기다려보자. / 정확도 0.3745 / 왜 똑같냐 / 정확도 잘못 측정한듯 다시 해야지
0124_03 : learning_late=0.0003 / 정확도 0.3070 / 비슷비슷하네

0125_01 : learning_late=0.0002 / conv&pool층 5개 + dense층 5개로 다시 / 아무리 돌려도 loss가 줄어들지 않는다... 포기.....
0125_02 : learning_late=0.0002 / conv4 + pool4 + dense5개로 수정 / 정확도 0.3165 / 조금 아쉽
0125_03 : learning_late=0.0003 / 정확도 0.2310
0125_04 : learning_late=0.0001 / 정확도 0.3085
0125_05 : learning_late=0.00015 / 정확도 0.3115
0125_06 : learning_late=0.00017 / 정확도 0.2520 / 층을 늘렸는데 왜 오히려 정확도가 낮아지는걸까... 일단 여기서 스탑.


==== 무신사 데이터셋 (추정) ======

0608_01 : 기존거 똑같이 conv4 + dense4인데 마지막을 1000-1000-100-50으로 변경. conv층도 마지막 2개는 conv층을 1개씩 더 추가 / learning_late=0.0002 / 정확도 0.2786 / 어째서어어어ㅓ
0608_02 : 구글넷 형식으로 변형 / learning_late=0.001 / 정확도 0.2692 / 어째서ㅓ어어어어ㅓ
0608_03 : learning_late=0.0002 / 정확도 0.2682
▲0608_04 : learning_late-0.00002 / 정확도 0.3004
0608_05 : learning_late=0.0002 / 레이어 구조 조금 더 변형 / 정확도 0.2849

▲0611_01 : vgg 경량 / 배경 안지운 버전 / learning_late=0.00002 / 정확도 0.4222

0613_01 : 구글넷 형식 / learnimg_late=0.00002 / 정확도 0.4057 / 흐으음
0613_02 : 엑스트라 네트워크 살리면 학습이 안된다. 학습률만 바꿔야지 / llearning_late=0.00001 / 정확도 0.3980 / 흐음?
0613_03 : learing_late=0.000005 / 정확도 0.3780 / 애매하다. 아예 층 구조를 바꿔야할듯
0613_04 : 인셉션 블록에 conv층 1개씩 추가 / learning_late=0.00002 / 정확도 0.3831
0613_05 : learining_late=0.00001 / 정확도 0.3884 / 애매하네...
0613_06 : 드롭아웃과 conv층 추가 / learning_late=0.00001 / 정확도 0.3457 / 오히려 줄었다.....

★0721_01 : 배경 크롭좌표하고 회전도 4번 해서 어그멘테이션. 구조는 시간 빠른 구글넷 / learning_late=0.00002 / 학습이 안돼서 모델 구조 간소화 / 층마다 드롭아웃이랑 conv층 섞어준거 없애주니까 다시 잘되네. 까탈스럽긴 / 정확도 0.3363 / 배경 안지운것보다는 낮긴한데 완전히 기본 데이터셋보다는 높은듯. 좀 더 조정해보자.

0725_01 : 구글넷에서 큰 층마다 드롭아웃 추가하고 마지막에 dense층 2개 추가. average 풀링 이후의 드롭아웃 비율을 더 낮게 조정 / learning_late=0.00002 / 정확도 0.2982 / 어째서!!!
★0725_02 : 구글넷인데 중간층 드롭아웃 다시 지울까. 마지막 dense층 2개는 유지 / learning_late=0.00002 / 정확도 0.3371 / 왜 자꾸 낮게 나오냐 / 실수로 삭제해버려서 다시 학습중인데 왜 그때의 정확도가 안나오냐
0725_03 : 구글넷 기본 구조처럼 돌아가자. 중간 드롭아웃, 마지막 dense층 없애고 초반 풀링 다시 살림 / learning_late=0.00002 / 정확도 0.3165 / 왜 계속 낮냐고 / 재학습 정확도 0.2955

0809_01 : 07225_02와 구조는 같은데 learning_late=0.00001로 조정 / 정확도 0.3014
0809_02 : vgg에서 모든 드롭아웃 비율을 0.2로 조정. pool4+dense3 / learning_late=0.00002 / 정확도 0.2727 . 애매하네
0809_03 : vgg인데 dense층 3개에서 4개로 증가 / learning_late=0.00002 /  정확도 0.2518 / 역시 안되는건가

0915_01 : 어그멘테이션X / 커널 크기를 입력 이미지 크기랑 동일하게 해서 유사 vgg 구조 / conv3+dense4 / learning_late=0.00002 / 필터 크기 줄였는데도 램 부족 오류 뜬다 포기...

0916_01 : 구글넷 흑백으로 테스트. 어그멘테이션 없음(램 오류). 구조는 0721_01과 동일 / learning_late=0.00002 / 정확도 0.3182
0916_02 : 구글넷 흑백. 구조는 0725_02와 동일하게 dense층 2개 추가. 어그멘테이션 없음 / learning_late=0.00002 / 정확도 0.3084

//
언젠가
0nnn_01 : vgg19 따라하자 / learning_late=0.001

///해야될거
VGG19 / 
GoogLeNet / 
DenseNet / 
ResNet / 
InceptionResNetV2 /
InceptionV3 / 
MobileNet /
NasNetMobile / 